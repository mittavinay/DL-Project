{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4c46262",
   "metadata": {},
   "source": [
    "# **Jowar plant leaf disease detection**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3654c9a",
   "metadata": {},
   "source": [
    "## **Overview<font color='#FFCC00'>  • </font>**\n",
    "\n",
    "The main aim of these project is to re-develope a new model for Jowar plant leaf disease detection, which is previosuly developed @Manivarsh-adi [github_page](https://github.com/Manivarsh-adi/Jowar_Plant_Leaf_Disease_Detection_Using_Deep_Learning)\n",
    "\n",
    "After going through [Paper](https://ieeexplore.ieee.org/abstract/document/9596535) and above github page it's evident developed model is failing on production data (test data). So our motive is to develope a new model to boost up the testing score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a2634f",
   "metadata": {},
   "source": [
    "## **Previous work overview<font color='#FFCC00'>  • </font>**\n",
    "\n",
    "[Paper](https://ieeexplore.ieee.org/abstract/document/9596535) defined a ResNet50 and self designed models to detect a jowar leaf is diagnosed with Athracnose and Leaf Blight or Healthy Leaf.\n",
    "\n",
    "Author used a self designed image segemention technique to highlight symptoms of Anthracnose and leaf blight using OpenCV, this technique is inspired from google maps segementation technique.\n",
    "\n",
    "![Segemented image](https://github.com/Manivarsh-adi/Jowar_Plant_Leaf_Disease_Detection_Using_Deep_Learning/blob/main/All_Segmentation.jpg?raw=true)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90663fb3",
   "metadata": {},
   "source": [
    "## **Project Motive**\n",
    "\n",
    "Our motive is to build a new models cosidering the base models like LeNet-5 and ResNet50 for binary detection on the dataset. As seen model is failing to achieve good accuracy due to multi class classification. So we decided to consider a binary classification model where data is labeled as 0 (Healthy) and 1 (Diagnosed).\n",
    "\n",
    " $0-->Healthy$ and  $1-->Diagnosed$\n",
    " \n",
    " Instead of Symptoms threshold technique used by Author, we decide to go with grey scale images. As seen above and dataset for some of the images background is also marking as symptoms like soil colour etc.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95e49a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras.layers import Flatten, BatchNormalization, Dense, Dropout\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "import keras\n",
    "#from keras.optimizers import Adam\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.regularizers import l1, l2, l1_l2\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
    "from tensorflow.keras import experimental, layers\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10d39d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e780bbda",
   "metadata": {},
   "source": [
    "## Reading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "880e361b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 323 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_dataset = image_dataset_from_directory(r\"C:\\Users\\adhim\\Desktop\\Jowar Compressed Segemented\\CompressedSegmentedGray\\Train\",\n",
    "                                          image_size = (224, 224),\n",
    "                                          #batch_size = 32,\n",
    "                                          shuffle = True\n",
    "                                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed13f4ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 87 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "val_dataset = image_dataset_from_directory( r\"C:\\Users\\adhim\\Desktop\\Jowar Compressed Segemented\\CompressedSegmentedGray\\Validation\",\n",
    "                                                image_size = (224, 224),\n",
    "                                                #batch_size = 32,\n",
    "                                                shuffle = True\n",
    "                                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "230d1ba7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 96 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "test_dataset = image_dataset_from_directory( r\"C:\\Users\\adhim\\Desktop\\test images\",\n",
    "                                                image_size = (224, 224),\n",
    "                                                #batch_size = 32,\n",
    "                                                shuffle = True\n",
    "                                          )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d3443d",
   "metadata": {},
   "source": [
    "## Splitting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab9d66ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = []\n",
    "y_train = []\n",
    "\n",
    "for image_batch, label_batch in train_dataset:\n",
    "    for image, label in zip(image_batch, label_batch):\n",
    "        x_train.append(np.array(image.numpy()).astype(np.float32))\n",
    "        y_train.append(np.array(label.numpy()).astype(np.int32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3048e592",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val = []\n",
    "y_val = []\n",
    "\n",
    "for image_batch, label_batch in val_dataset:\n",
    "     for image, label in zip(image_batch, label_batch):\n",
    "            x_val.append(np.array(image.numpy()).astype(np.float32))\n",
    "            y_val.append(np.array(label.numpy()).astype(np.int32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "936a1806",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = []\n",
    "y_test = []\n",
    "\n",
    "for image_batch, label_batch in test_dataset:\n",
    "     for image, label in zip(image_batch, label_batch):\n",
    "            x_test.append(np.array(image.numpy()).astype(np.float32))\n",
    "            y_test.append(np.array(label.numpy()).astype(np.int32))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe6113e",
   "metadata": {},
   "source": [
    "## Replicating LeNet-5\n",
    "\n",
    "Here our work is to replicate LeNet-5 to pre-train the dataset on this model.\n",
    "\n",
    "### Lets look layer to layer feature map process\n",
    "\n",
    "C1 layer-convolutional layer:\n",
    "* Input picture: 250 * 250\n",
    "* Convolution kernel size: 5 * 5\n",
    "* Convolution kernel types: 6\n",
    "* Output featuremap size: 246 * 246 (250 - 5 + 1) = 246\n",
    "* Number of neurons: 246 246 6\n",
    "\n",
    "S2 layer-pooling layer (downsampling layer):\n",
    "* Input: 246 * 246\n",
    "* Sampling area: 2 * 2\n",
    "* Sampling method: 4 inputs are added, multiplied by a trainable parameter, plus a trainable offset. Results via sigmoid\n",
    "* Sampling type: 6\n",
    "* Output featureMap size: 123 * 123 (123/2)\n",
    "* Number of neurons: 123 123 6\n",
    "\n",
    "C3 layer-convolutional layer:\n",
    "* Input: all 6 or several feature map combinations in S2\n",
    "* Convolution kernel size: 5 * 5\n",
    "* Convolution kernel type: 16\n",
    "* Output featureMap size: 10 * 10 (123 - 5 + 1) = 118\n",
    "* Number of neurons: 118 118 16\n",
    "\n",
    "S4 layer-pooling layer (downsampling layer)\n",
    "* Input: 118 * 118\n",
    "* Sampling area: 2 * 2\n",
    "* Sampling method: 4 inputs are added, multiplied by a trainable parameter, plus a trainable offset. Results via sigmoid\n",
    "* Sampling type: 16\n",
    "* Output featureMap size: 59 * 59 (118/2)\n",
    "* Number of neurons: 59 59 16\n",
    "\n",
    "C5 layer-convolution layer\n",
    "* Input: All 16 unit feature maps of the S4 layer (all connected to s4)\n",
    "* Convolution kernel size: 5 * 5\n",
    "* Convolution kernel type: 120\n",
    "* Output featureMap size: 55 * 55 (59-5 + 1)\n",
    "* Number of neurons: 55 55 120"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "07b349a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "leaky_relu_alpha = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5cac99d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "51ac388d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(tf.keras.layers.experimental.preprocessing.RandomFlip('horizontal'))\n",
    "\n",
    "model.add(tf.keras.layers.experimental.preprocessing.RandomRotation(0.2))\n",
    "\n",
    "model.add(layers.Conv2D(filters = 6, kernel_size = (5, 5), strides = (1,1), padding = 'valid', activation = keras.activations.tanh, input_shape = (250, 250, 1)))\n",
    "\n",
    "model.add(layers.MaxPool2D( pool_size = (2, 2), strides = (2, 2)))\n",
    "\n",
    "model.add(layers.Conv2D(filters = 16, kernel_size = (5, 5), strides = (1, 1), padding = 'valid', activation = keras.activations.tanh))\n",
    "\n",
    "model.add(layers.MaxPool2D( pool_size = (2, 2), strides = (2, 2)))\n",
    "\n",
    "model.add(layers.Conv2D(filters = 120, kernel_size = (5, 5), strides = (1,1), padding = 'valid', activation = keras.activations.tanh))\n",
    "\n",
    "model.add(layers.Flatten())\n",
    "\n",
    "model.add(layers.Dense(84, activation = keras.activations.tanh))\n",
    "\n",
    "model.add(layers.Dense(1, activation = keras.activations.softmax))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ca34d763",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=keras.metrics.binary_crossentropy, optimizer=keras.optimizers.Adam(), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5b724a4b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "11/11 [==============================] - 15s 281ms/step - loss: 2.8706 - accuracy: 0.7399 - val_loss: 3.6404 - val_accuracy: 0.6897\n",
      "Epoch 2/20\n",
      "11/11 [==============================] - 2s 156ms/step - loss: 2.9558 - accuracy: 0.7399 - val_loss: 3.3624 - val_accuracy: 0.6897\n",
      "Epoch 3/20\n",
      "11/11 [==============================] - 2s 168ms/step - loss: 2.7242 - accuracy: 0.7399 - val_loss: 3.0917 - val_accuracy: 0.6897\n",
      "Epoch 4/20\n",
      "11/11 [==============================] - 2s 157ms/step - loss: 2.5020 - accuracy: 0.7399 - val_loss: 2.8308 - val_accuracy: 0.6897\n",
      "Epoch 5/20\n",
      "11/11 [==============================] - 2s 163ms/step - loss: 2.2791 - accuracy: 0.7399 - val_loss: 2.5537 - val_accuracy: 0.6897\n",
      "Epoch 6/20\n",
      "11/11 [==============================] - 2s 155ms/step - loss: 2.0390 - accuracy: 0.7399 - val_loss: 2.2703 - val_accuracy: 0.6897\n",
      "Epoch 7/20\n",
      "11/11 [==============================] - 2s 170ms/step - loss: 1.8093 - accuracy: 0.7399 - val_loss: 1.9920 - val_accuracy: 0.6897\n",
      "Epoch 8/20\n",
      "11/11 [==============================] - 2s 170ms/step - loss: 1.5788 - accuracy: 0.7399 - val_loss: 1.7222 - val_accuracy: 0.6897\n",
      "Epoch 9/20\n",
      "11/11 [==============================] - 2s 153ms/step - loss: 1.3592 - accuracy: 0.7399 - val_loss: 1.4723 - val_accuracy: 0.6897\n",
      "Epoch 10/20\n",
      "11/11 [==============================] - 2s 161ms/step - loss: 1.1446 - accuracy: 0.7399 - val_loss: 1.2205 - val_accuracy: 0.6897\n",
      "Epoch 11/20\n",
      "11/11 [==============================] - 2s 156ms/step - loss: 0.9319 - accuracy: 0.7399 - val_loss: 0.9787 - val_accuracy: 0.6897\n",
      "Epoch 12/20\n",
      "11/11 [==============================] - 2s 153ms/step - loss: 0.7695 - accuracy: 0.7399 - val_loss: 0.7856 - val_accuracy: 0.6897\n",
      "Epoch 13/20\n",
      "11/11 [==============================] - 2s 154ms/step - loss: 0.6405 - accuracy: 0.7399 - val_loss: 0.6861 - val_accuracy: 0.6897\n",
      "Epoch 14/20\n",
      "11/11 [==============================] - 2s 161ms/step - loss: 0.5861 - accuracy: 0.7399 - val_loss: 0.6389 - val_accuracy: 0.6897\n",
      "Epoch 15/20\n",
      "11/11 [==============================] - 2s 153ms/step - loss: 0.5734 - accuracy: 0.7399 - val_loss: 0.6270 - val_accuracy: 0.6897\n",
      "Epoch 16/20\n",
      "11/11 [==============================] - 2s 165ms/step - loss: 0.5730 - accuracy: 0.7399 - val_loss: 0.6223 - val_accuracy: 0.6897\n",
      "Epoch 17/20\n",
      "11/11 [==============================] - 2s 148ms/step - loss: 0.5747 - accuracy: 0.7399 - val_loss: 0.6204 - val_accuracy: 0.6897\n",
      "Epoch 18/20\n",
      "11/11 [==============================] - 2s 168ms/step - loss: 0.5763 - accuracy: 0.7399 - val_loss: 0.6200 - val_accuracy: 0.6897\n",
      "Epoch 19/20\n",
      "11/11 [==============================] - 2s 153ms/step - loss: 0.5756 - accuracy: 0.7399 - val_loss: 0.6214 - val_accuracy: 0.6897\n",
      "Epoch 20/20\n",
      "11/11 [==============================] - 2s 154ms/step - loss: 0.5746 - accuracy: 0.7399 - val_loss: 0.6240 - val_accuracy: 0.6897\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1b588c79670>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_dataset, batch_size=128, epochs=20, verbose=1, validation_data=val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255c8d1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "532ecf45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 - 0s - loss: 0.6893 - accuracy: 0.6250\n",
      "\n",
      "Test accuracy: 0.625\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(test_dataset, verbose=2)\n",
    "\n",
    "print('\\nTest accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a3a86fb",
   "metadata": {},
   "source": [
    "## Fine tuning ResNet50 \n",
    "\n",
    "Here our aim id to fine tune ResNet50 with binary grey scale image data using transer learning mechanism.\n",
    "\n",
    "1. Firslty, need to load renet model from keras module by mentioning input size and as well as which weights need to consider to fine tune in this case we considered ever green \"imagenet\" weights. <br>\n",
    "2. Using transfer learning mechanism we can allow some of the top layers to update during fin tuning process, here we allowed top two layers 144 and 145 layers to update during fine tuning.<br>\n",
    "3. Then we will load sequential model by adding all necassery layers to it.\n",
    "4. To make dataset large we try to make some flips on data which creates a new images for each image.\n",
    "5. Then loaded resnet model is added to sequential layer.\n",
    "6. Following added flatten layer to flatten the input to feed forward network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e6c224",
   "metadata": {},
   "source": [
    "## ResNet-50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f9bfd66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_x = tf.keras.Input( shape = (224, 224, 3))\n",
    "\n",
    "resnet_model = tf.keras.applications.ResNet50(include_top = False,\n",
    "                                               weights = 'imagenet',\n",
    "                                               input_tensor = input_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5ad819a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for layers in resnet_model.layers[:143]:\n",
    "    \n",
    "    layers.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c30f5604",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = models.Sequential()\n",
    "\n",
    "model2.add(tf.keras.layers.experimental.preprocessing.RandomFlip('horizontal'))\n",
    "\n",
    "model2.add(tf.keras.layers.experimental.preprocessing.RandomRotation(0.2))\n",
    "\n",
    "model2.add(resnet_model)\n",
    "\n",
    "model2.add(Flatten())\n",
    "\n",
    "#model2.add(Dense(84, activation = keras.activations.tanh))\n",
    "\n",
    "#model2.add(Dense(1, activation = keras.activations.softmax))\n",
    "\n",
    "model2.add(BatchNormalization())\n",
    "\n",
    "model2.add(Dense(256, activation = 'relu'))\n",
    "\n",
    "model2.add(Dropout(0.5))\n",
    "\n",
    "model2.add(BatchNormalization())\n",
    "\n",
    "model2.add(Dense(128, activation = 'relu'))\n",
    "\n",
    "model2.add(Dropout(0.5))\n",
    "\n",
    "model2.add(BatchNormalization())\n",
    "\n",
    "model2.add(Dense(64, activation = 'relu'))\n",
    "\n",
    "model2.add(Dropout(0.5))\n",
    "\n",
    "model2.add(BatchNormalization())\n",
    "\n",
    "model2.add(Dense(1, activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0c55664c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.compile(loss = keras.metrics.binary_crossentropy,\n",
    "              optimizer = 'adam',\n",
    "              metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "adf7f771",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "11/11 [==============================] - 21s 1s/step - loss: 0.8403 - accuracy: 0.7399 - val_loss: 1.1354 - val_accuracy: 0.6897\n",
      "Epoch 2/20\n",
      "11/11 [==============================] - 9s 789ms/step - loss: 0.6968 - accuracy: 0.7399 - val_loss: 3.3837 - val_accuracy: 0.6897\n",
      "Epoch 3/20\n",
      "11/11 [==============================] - 9s 778ms/step - loss: 0.6636 - accuracy: 0.7399 - val_loss: 9.9357 - val_accuracy: 0.6897\n",
      "Epoch 4/20\n",
      "11/11 [==============================] - 9s 780ms/step - loss: 0.5790 - accuracy: 0.7399 - val_loss: 12.0524 - val_accuracy: 0.6897\n",
      "Epoch 5/20\n",
      "11/11 [==============================] - 9s 778ms/step - loss: 0.4917 - accuracy: 0.7399 - val_loss: 7.4145 - val_accuracy: 0.6897\n",
      "Epoch 6/20\n",
      "11/11 [==============================] - 9s 777ms/step - loss: 0.5315 - accuracy: 0.7399 - val_loss: 9.6291 - val_accuracy: 0.6897\n",
      "Epoch 7/20\n",
      "11/11 [==============================] - 9s 776ms/step - loss: 0.5076 - accuracy: 0.7399 - val_loss: 6.3884 - val_accuracy: 0.6897\n",
      "Epoch 8/20\n",
      "11/11 [==============================] - 9s 776ms/step - loss: 0.4779 - accuracy: 0.7399 - val_loss: 20.9579 - val_accuracy: 0.6897\n",
      "Epoch 9/20\n",
      "11/11 [==============================] - 9s 777ms/step - loss: 0.4111 - accuracy: 0.7399 - val_loss: 7.5301 - val_accuracy: 0.6897\n",
      "Epoch 10/20\n",
      "11/11 [==============================] - 9s 777ms/step - loss: 0.4147 - accuracy: 0.7399 - val_loss: 5.1150 - val_accuracy: 0.6897\n",
      "Epoch 11/20\n",
      "11/11 [==============================] - 9s 778ms/step - loss: 0.3850 - accuracy: 0.7399 - val_loss: 2.0973 - val_accuracy: 0.6897\n",
      "Epoch 12/20\n",
      "11/11 [==============================] - 9s 777ms/step - loss: 0.4061 - accuracy: 0.7399 - val_loss: 2.3041 - val_accuracy: 0.6897\n",
      "Epoch 13/20\n",
      "11/11 [==============================] - 9s 774ms/step - loss: 0.3754 - accuracy: 0.7399 - val_loss: 2.7791 - val_accuracy: 0.6897\n",
      "Epoch 14/20\n",
      "11/11 [==============================] - 9s 775ms/step - loss: 0.3230 - accuracy: 0.7399 - val_loss: 2.7562 - val_accuracy: 0.6897\n",
      "Epoch 15/20\n",
      "11/11 [==============================] - 9s 776ms/step - loss: 0.3260 - accuracy: 0.7399 - val_loss: 2.1324 - val_accuracy: 0.6897\n",
      "Epoch 16/20\n",
      "11/11 [==============================] - 9s 773ms/step - loss: 0.3110 - accuracy: 0.7399 - val_loss: 2.6559 - val_accuracy: 0.6897\n",
      "Epoch 17/20\n",
      "11/11 [==============================] - 9s 779ms/step - loss: 0.3048 - accuracy: 0.7399 - val_loss: 1.6423 - val_accuracy: 0.6897\n",
      "Epoch 18/20\n",
      "11/11 [==============================] - 9s 775ms/step - loss: 0.2724 - accuracy: 0.7399 - val_loss: 1.4905 - val_accuracy: 0.6897\n",
      "Epoch 19/20\n",
      "11/11 [==============================] - 9s 775ms/step - loss: 0.2865 - accuracy: 0.7399 - val_loss: 1.2011 - val_accuracy: 0.6897\n",
      "Epoch 20/20\n",
      "11/11 [==============================] - 9s 774ms/step - loss: 0.2823 - accuracy: 0.7399 - val_loss: 0.9975 - val_accuracy: 0.6897\n"
     ]
    }
   ],
   "source": [
    "history = model2.fit(train_dataset,\n",
    "                    batch_size = 32,\n",
    "                    epochs = 20,\n",
    "                    validation_data = val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "927cb278",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 - 0s - loss: 0.5662 - accuracy: 0.6250\n",
      "\n",
      "Test accuracy: 0.625\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model2.evaluate(test_dataset, verbose=2)\n",
    "\n",
    "print('\\nTest accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5710b203",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
